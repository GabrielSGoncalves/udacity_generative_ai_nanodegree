{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9380179-fa98-44f0-991f-0fdeb07990ea",
   "metadata": {},
   "source": [
    "# Udacity GenAI Nanodegreee Project 1: Applying Lightweight Fine-Tuning to a Foundation Model\n",
    "## Overview\n",
    "Large Language Models can be expensive to train, as it requires lots of computing resources. One approach to avoid training the whole model for a specific purpose is by leveraging Parameter-Efficient Fine-Tuning (PEFT) methods, that enables efficient training of a small portion of the model.\n",
    "\n",
    "In some cases, using PEFT can have a comparable performance to training a full LLM from scratch, with a fraction of the cost.\n",
    "\n",
    "## Project objective\n",
    "- The objective of this use PEFT to train a LLM \n",
    "\n",
    "## Practical approach\n",
    "In this project, you will bring together all of the essential components of a PyTorch + Hugging Face training and inference process. Specifically, you will:\n",
    "\n",
    "1. Load a pre-trained model and evaluate its performance\n",
    "2. Perform parameter-efficient fine tuning using the pre-trained model\n",
    "3. Perform inference using the fine-tuned model and compare its performance to the original model\n",
    "\n",
    "## References\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee9cdb6-8f53-48c8-8a2b-9a7aa3f156e0",
   "metadata": {},
   "source": [
    "# Loading base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40e24671-e1bd-414d-b8a5-6e668fbe63d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabrielsgoncalves/Documents/Repositories/udacity_generative_ai_nanodegree/project1_fine_tuning_foundational_model/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "333637ee-e906-43f9-af6d-d47a8211fb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "num_labels = 2  # Binary classification\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "097effbf-2f94-4877-ac5a-c5ca307c428e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load base model\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bbee36-a7e5-4385-b170-58c2507be76f",
   "metadata": {},
   "source": [
    "# Loading dataset\n",
    "We are going to use IMDB full dataset with 25.000 samples for each split (train and validate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3cba790-536d-448e-b4fc-de8323b9fd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare dataset (using IMDB for sentiment analysis)\n",
    "dataset = load_dataset(\"imdb\", split=['train[:25000]', 'test[:25000]'])\n",
    "train_dataset, eval_dataset = dataset[0], dataset[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccf75573-c104-4342-9a1a-3ab214ead804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (25_000, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>text</th><th>label</th></tr><tr><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;I rented I AM CURIOUS-YELLOW f…</td><td>0</td></tr><tr><td>&quot;&quot;I Am Curious: Yellow&quot; is a ri…</td><td>0</td></tr><tr><td>&quot;If only to avoid making this t…</td><td>0</td></tr><tr><td>&quot;This film was probably inspire…</td><td>0</td></tr><tr><td>&quot;Oh, brother...after hearing ab…</td><td>0</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;A hit at the time but now bett…</td><td>1</td></tr><tr><td>&quot;I love this movie like no othe…</td><td>1</td></tr><tr><td>&quot;This film and it&#x27;s sequel Barr…</td><td>1</td></tr><tr><td>&quot;&#x27;The Adventures Of Barry McKen…</td><td>1</td></tr><tr><td>&quot;The story centers around Barry…</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (25_000, 2)\n",
       "┌─────────────────────────────────┬───────┐\n",
       "│ text                            ┆ label │\n",
       "│ ---                             ┆ ---   │\n",
       "│ str                             ┆ i64   │\n",
       "╞═════════════════════════════════╪═══════╡\n",
       "│ I rented I AM CURIOUS-YELLOW f… ┆ 0     │\n",
       "│ \"I Am Curious: Yellow\" is a ri… ┆ 0     │\n",
       "│ If only to avoid making this t… ┆ 0     │\n",
       "│ This film was probably inspire… ┆ 0     │\n",
       "│ Oh, brother...after hearing ab… ┆ 0     │\n",
       "│ …                               ┆ …     │\n",
       "│ A hit at the time but now bett… ┆ 1     │\n",
       "│ I love this movie like no othe… ┆ 1     │\n",
       "│ This film and it's sequel Barr… ┆ 1     │\n",
       "│ 'The Adventures Of Barry McKen… ┆ 1     │\n",
       "│ The story centers around Barry… ┆ 1     │\n",
       "└─────────────────────────────────┴───────┘"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].to_polars()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af4ddb2-f77b-4bc9-875a-e7d5a5d2ff37",
   "metadata": {},
   "source": [
    "# Tokenizing the text samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "809ec96f-5af0-4755-9785-54ef1dc3c3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████| 25000/25000 [00:03<00:00, 8258.68 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        truncation=True, \n",
    "        padding=True, \n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval = eval_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1c86b5-25fb-4ef2-a8ad-21051a61963d",
   "metadata": {},
   "source": [
    "# Defining LoRA Configuration and PEFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76258dc0-3446-4896-8196-c2849ae8b30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=8,  # rank\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"c_proj\", \"c_attn\"],\n",
    "    bias=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b1b59b2-5d4b-477d-8ff3-865d7c47069f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 812,544 || all params: 125,253,888 || trainable%: 0.6487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabrielsgoncalves/Documents/Repositories/udacity_generative_ai_nanodegree/project1_fine_tuning_foundational_model/.venv/lib/python3.12/site-packages/peft/tuners/lora/layer.py:1264: UserWarning:\n",
      "\n",
      "fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create PEFT model\n",
    "peft_model = get_peft_model(base_model, peft_config)\n",
    "peft_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e080fdb-57b6-4199-982c-2a5a6ed68888",
   "metadata": {},
   "source": [
    "# Training PEFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b2776a1-2ae2-4853-8216-776ccc05ac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4258f238-f67a-429b-8d99-cb9ec4ace39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"gpt2-imdb-peft\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f64c639-47ea-4106-8999-7c8e478e7a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-04 00:24:08,578] [WARNING] [real_accelerator.py:181:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
      "[2025-03-04 00:24:08,581] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f0afc22-2c27-4ef8-ac73-4caa42d10691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9375' max='9375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9375/9375 17:20:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.273700</td>\n",
       "      <td>0.220350</td>\n",
       "      <td>0.926600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.278500</td>\n",
       "      <td>0.245032</td>\n",
       "      <td>0.925920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>0.242862</td>\n",
       "      <td>0.933840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9375, training_loss=0.2927575398763021, metrics={'train_runtime': 62423.8985, 'train_samples_per_second': 1.201, 'train_steps_per_second': 0.15, 'total_flos': 1.9784466432e+16, 'train_loss': 0.2927575398763021, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62985199-caf5-4f2d-9e81-258087b5c4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the PEFT model\n",
    "peft_model.save_pretrained(\"gpt2-imdb-peft/best_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77585487-5d22-4a89-a80d-3e7f9f6965e5",
   "metadata": {},
   "source": [
    "# Inference\n",
    "In this session we are going to define how to perform inferences for both base model, GPT2, and the fine tunned we trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "157aadcb-1940-4992-9a99-dd1332654193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForSequenceClassification\n",
    "\n",
    "# Load the fine-tuned model\n",
    "loaded_peft_model = AutoPeftModelForSequenceClassification.from_pretrained(\n",
    "    \"gpt2-imdb-peft/best_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0272056-84ed-43e7-bf40-bc5aba105938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference function\n",
    "def predict_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = loaded_peft_model(**inputs)\n",
    "    prediction = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "    return \"Positive\" if prediction[0][1] > prediction[0][0] else \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93f5fe4c-bae9-4d5d-bd6e-a610b7333726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "test_text = \"This movie was absolutely fantastic! I loved every minute of it.\"\n",
    "print(f\"Sentiment: {predict_sentiment(test_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7ef916-8ff2-4d0f-92e2-dd3ad38a2bee",
   "metadata": {},
   "source": [
    "# Evaluating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3aa96f2-5652-48ff-80aa-6efa797cf429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, tokenizer):\n",
    "    # Set up trainer for evaluation only\n",
    "    eval_trainer = Trainer(\n",
    "        model=model,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=\"./eval_output\",\n",
    "            per_device_eval_batch_size=8,\n",
    "            remove_unused_columns=True,\n",
    "        ),\n",
    "        eval_dataset=dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    )\n",
    "    \n",
    "    # Run evaluation\n",
    "    eval_results = eval_trainer.evaluate()\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50e29d3-bdbb-4197-a0cc-9fcde8dd219a",
   "metadata": {},
   "source": [
    "## Evaluating GPT2 base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0737e8-f878-4529-bd58-78ee975a6afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating base GPT-2 model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1725' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1725/3125 39:32 < 32:06, 0.73 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate base model\n",
    "print(\"Evaluating base GPT-2 model...\")\n",
    "base_model_results = evaluate_model(base_model, tokenized_eval, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a193bd1-4f40-4bd3-953c-2eeac1d0df2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PEFT model\n",
    "print(\"Evaluating PEFT fine-tuned model...\")\n",
    "peft_model_results = evaluate_model(loaded_peft_model, tokenized_eval, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf40105-56ae-44a8-8838-4d96778d2288",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compare results\n",
    "print(\"\\n=== Model Comparison ===\")\n",
    "print(f\"Base GPT-2 Accuracy: {base_model_results['eval_accuracy']:.4f}\")\n",
    "print(f\"PEFT Fine-tuned Accuracy: {peft_model_results['eval_accuracy']:.4f}\")\n",
    "print(f\"Improvement: {(peft_model_results['eval_accuracy'] - base_model_results['eval_accuracy'])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570d4fd7-d8aa-4945-9a2b-8af9d13fb39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "print(\"\\n=== Model Comparison ===\")\n",
    "print(f\"Base GPT-2 Accuracy: {base_model_results['eval_accuracy']:.4f}\")\n",
    "print(f\"PEFT Fine-tuned Accuracy: {peft_model_results['eval_accuracy']:.4f}\")\n",
    "print(f\"Improvement: {(peft_model_results['eval_accuracy'] - base_model_results['eval_accuracy'])*100:.2f}%\")\n",
    "\n",
    "# Optional: Detailed comparison of specific examples\n",
    "def compare_predictions(model1, model2, examples, tokenizer, num_samples=5):\n",
    "    print(\"\\n=== Sample Predictions Comparison ===\")\n",
    "    for i in range(min(num_samples, len(examples))):\n",
    "        text = examples[i][\"text\"]\n",
    "        true_label = \"Positive\" if examples[i][\"label\"] == 1 else \"Negative\"\n",
    "        \n",
    "        # Get predictions\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Base model prediction\n",
    "            base_outputs = model1(**inputs)\n",
    "            base_pred = torch.nn.functional.softmax(base_outputs.logits, dim=1)\n",
    "            base_sentiment = \"Positive\" if base_pred[0][1] > base_pred[0][0] else \"Negative\"\n",
    "            \n",
    "            # PEFT model prediction\n",
    "            peft_outputs = model2(**inputs)\n",
    "            peft_pred = torch.nn.functional.softmax(peft_outputs.logits, dim=1)\n",
    "            peft_sentiment = \"Positive\" if peft_pred[0][1] > peft_pred[0][0] else \"Negative\"\n",
    "        \n",
    "        print(f\"Example {i+1}:\")\n",
    "        print(f\"Text: {text[:100]}...\")\n",
    "        print(f\"True label: {true_label}\")\n",
    "        print(f\"Base model prediction: {base_sentiment} (confidence: {max(base_pred[0]).item():.2f})\")\n",
    "        print(f\"PEFT model prediction: {peft_sentiment} (confidence: {max(peft_pred[0]).item():.2f})\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Get a few examples from the validation set\n",
    "sample_examples = [eval_dataset[i] for i in range(5)]\n",
    "compare_predictions(base_model, loaded_peft_model, sample_examples, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab69022-87ac-490f-9888-f66fc7422b92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
