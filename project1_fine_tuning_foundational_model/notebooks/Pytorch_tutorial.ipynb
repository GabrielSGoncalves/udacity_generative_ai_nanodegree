{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6526b194-8084-459b-83fb-ba7ec7fd7ff0",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e130e4f-fce5-4a50-bf4f-4a4f1917906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a 3-dimensional tensor\n",
    "images = torch.rand((4, 28, 28))\n",
    "\n",
    "# Get the second image\n",
    "second_image = images[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44ce18ad-4bcd-4638-b915-dae58068aba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(second_image, cmap='gray')\n",
    "plt.axis('off') # disable axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "334964a2-0b36-4d83-98de-c80f23374b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1],\n",
      "        [1, 0]])\n",
      "tensor([[2, 1],\n",
      "        [1, 1]])\n",
      "tensor([[3, 2],\n",
      "        [2, 1]])\n",
      "tensor([[5, 3],\n",
      "        [3, 2]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 1], [1, 0]])\n",
    "\n",
    "print(a)\n",
    "# tensor([[1, 1],\n",
    "#         [1, 0]])\n",
    "\n",
    "print(torch.matrix_power(a, 2))\n",
    "# tensor([[2, 1],\n",
    "#         [1, 1]])\n",
    "\n",
    "print(torch.matrix_power(a, 3))\n",
    "# tensor([[3, 2],\n",
    "#         [2, 1]])\n",
    "\n",
    "print(torch.matrix_power(a, 4))\n",
    "# tensor([[5, 3],\n",
    "#         [3, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23f62a39-61dc-4ea2-8793-0e36ca3b2aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (hidden_layer): Linear(in_features=10, out_features=64, bias=True)\n",
      "  (output_layer): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (activation): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super(MLP, self).__init__()\n",
    "            self.hidden_layer = nn.Linear(input_size, 64)\n",
    "            self.output_layer = nn.Linear(64, 2)\n",
    "            self.activation = nn.ReLU()\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.activation(self.hidden_layer(x))\n",
    "            return self.output_layer(x)\n",
    "\n",
    "model = MLP(input_size=10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80bbf0e5-e979-489f-aa13-4df27f3176e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NumberSumDataset(Dataset):\n",
    "    def __init__(self, data_range=(1, 10)):\n",
    "        self.numbers = list(range(data_range[0], data_range[1]))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        number1 = float(self.numbers[index // len(self.numbers)])\n",
    "        number2 = float(self.numbers[index % len(self.numbers)])\n",
    "        return torch.tensor([number1, number2]), torch.tensor([number1 + number2])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.numbers) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1de92788-9fea-45b7-88e8-e5d22f76519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NumberSumDataset(data_range=(1, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb743717-858a-4265-9b51-063051114887",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_layer = nn.Linear(input_size, 128)\n",
    "        self.output_layer = nn.Linear(128, 1)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.hidden_layer(x))\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f250821-a86a-4685-8c54-4b5cf2a20260",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NumberSumDataset(data_range=(0, 100))\n",
    "dataloader = DataLoader(dataset, batch_size=100, shuffle=True)\n",
    "model = MLP(input_size=2)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0053f1d-4a6f-41e2-b09f-fe2c07e9b610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Sum of Batch Losses = 2.48907\n",
      "Epoch 1: Sum of Batch Losses = 2.07578\n",
      "Epoch 2: Sum of Batch Losses = 1.67930\n",
      "Epoch 3: Sum of Batch Losses = 1.38857\n",
      "Epoch 4: Sum of Batch Losses = 1.22586\n",
      "Epoch 5: Sum of Batch Losses = 1.10144\n",
      "Epoch 6: Sum of Batch Losses = 1.01609\n",
      "Epoch 7: Sum of Batch Losses = 0.95442\n",
      "Epoch 8: Sum of Batch Losses = 0.89759\n",
      "Epoch 9: Sum of Batch Losses = 0.83159\n",
      "Epoch 10: Sum of Batch Losses = 0.78585\n",
      "Epoch 11: Sum of Batch Losses = 0.73415\n",
      "Epoch 12: Sum of Batch Losses = 0.69346\n",
      "Epoch 13: Sum of Batch Losses = 0.65700\n",
      "Epoch 14: Sum of Batch Losses = 0.60899\n",
      "Epoch 15: Sum of Batch Losses = 0.58250\n",
      "Epoch 16: Sum of Batch Losses = 0.54761\n",
      "Epoch 17: Sum of Batch Losses = 0.51561\n",
      "Epoch 18: Sum of Batch Losses = 0.48387\n",
      "Epoch 19: Sum of Batch Losses = 0.45854\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    total_loss = 0.0\n",
    "    for number_pairs, sums in dataloader:  # Iterate over the batches\n",
    "        predictions = model(number_pairs)  # Compute the model output\n",
    "        loss = loss_function(predictions, sums)  # Compute the loss\n",
    "        loss.backward()  # Perform backpropagation\n",
    "        optimizer.step()  # Update the parameters\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        total_loss += loss.item()  # Add the loss for all batches\n",
    "\n",
    "    # Print the loss for this epoch\n",
    "    print(\"Epoch {}: Sum of Batch Losses = {:.5f}\".format(epoch, total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ae8c4c3-5f56-48aa-afbe-38f37e0d8463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.9948], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([3.0, 7.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa3fcfa-e2d2-40ca-b96a-e355fbb88c43",
   "metadata": {},
   "source": [
    "# Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5d56aa0-074a-4afb-a008-119e607e1cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabrielsgoncalves/Documents/Repositories/udacity_generative_ai_nanodegree/project1_fine_tuning_foundational_model/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# See how many tokens are in the vocabulary\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02998fd2-6383-477a-9dcc-c2f0b445cc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'heart', 'genera', '##tive', 'ai']\n",
      "[1045, 2540, 11416, 6024, 9932]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the sentence\n",
    "tokens = tokenizer.tokenize(\"I heart Generative AI\")\n",
    "\n",
    "# Print the tokens\n",
    "print(tokens)\n",
    "# ['i', 'heart', 'genera', '##tive', 'ai']\n",
    "\n",
    "# Show the token ids assigned to each token\n",
    "print(tokenizer.convert_tokens_to_ids(tokens))\n",
    "# [1045, 2540, 11416, 6024, 9932]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f40834c-295d-4ea1-b206-6afac5f8630d",
   "metadata": {},
   "source": [
    "# Hugging face models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d3757eb-2e99-4082-adca-bc92ca9f5fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabrielsgoncalves/Documents/Repositories/udacity_generative_ai_nanodegree/project1_fine_tuning_foundational_model/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI love Generative AI\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Make prediction\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     13\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     14\u001b[0m     probabilities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load a pre-trained sentiment analysis model\n",
    "model_name = \"textattack/bert-base-uncased-imdb\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Tokenize the input sequence\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer(\"I love Generative AI\", return_tensors=\"pt\")\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs).logits\n",
    "    probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b591c1d2-57c3-4c5c-847a-d52c96831464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sentiment result\n",
    "if predicted_class == 1:\n",
    "    print(f\"Sentiment: Positive ({probabilities[0][1] * 100:.2f}%)\")\n",
    "else:\n",
    "    print(f\"Sentiment: Negative ({probabilities[0][0] * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38550a62-9d08-4670-b555-381f012eb883",
   "metadata": {},
   "source": [
    "# Huggingface Datasets library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94f53784-ad8d-4e33-b886-bce3fb8d0802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|███████████████████████████████| 25000/25000 [00:00<00:00, 325583.04 examples/s]\n",
      "Generating test split: 100%|████████████████████████████████| 25000/25000 [00:00<00:00, 390600.93 examples/s]\n",
      "Generating unsupervised split: 100%|████████████████████████| 50000/50000 [00:00<00:00, 412955.26 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "WARNING: This review contains SPOILERS. Do not read if you don't want some points revealed to you before you watch the film.<br /><br />With a cast like this, you wonder whether or not the actors and actresses knew exactly what they were getting into. Did they see the script and say, `Hey, Close Encounters of the Third Kind was such a hit that this one can't fail.' Unfortunately, it does. Did they even think to check on the director's credentials..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Load the IMDB dataset, which contains movie reviews\n",
    "# and sentiment labels (positive or negative)\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Fetch a revie from the training set\n",
    "review_number = 42\n",
    "sample_review = dataset[\"train\"][review_number]\n",
    "\n",
    "display(HTML(sample_review[\"text\"][:450] + \"...\"))\n",
    "# WARNING: This review contains SPOILERS. Do not read if you don't want some points revealed to you before you watch the\n",
    "# film.\n",
    "# \n",
    "# With a cast like this, you wonder whether or not the actors and actresses knew exactly what they were getting into. Did they\n",
    "# see the script and say, `Hey, Close Encounters of the Third Kind was such a hit that this one can't fail.' Unfortunately, it does.\n",
    "# Did they even think to check on the director's credentials...\n",
    "\n",
    "if sample_review[\"label\"] == 1:\n",
    "    print(\"Sentiment: Positive\")\n",
    "else:\n",
    "    print(\"Sentiment: Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4edb7523-b575-47d8-a491-19b9a150bda1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"WARNING: This review contains SPOILERS. Do not read if you don't want some points revealed to you before you watch the film.<br /><br />With a cast like this, you wonder whether or not the actors and actresses knew exactly what they were getting into. Did they see the script and say, `Hey, Close Encounters of the Third Kind was such a hit that this one can't fail.' Unfortunately, it does. Did they even think to check on the director's credentials? I mean, would YOU do a movie with the director of a movie called `Satan's Cheerleaders?' Greydon Clark, who would later go on to direct the infamous `Final Justice,' made this. It makes you wonder how the people of Mystery Science Theater 3000 could hammer `Final Justice' and completely miss out on `The Return.'<br /><br />The film is set in a small town in New Mexico. A little boy and girl are in the street unsupervised one night when a powerful flashlight beam.er.a spaceship appears and hovers over them. In probably the worst special effect sequence of the film, the ship spews some kind of red ink on them. It looked like Clark had held a beaker of water in from of the camera lens and dipped his leaky pen in it, so right away you are treated with cheese. Anyhow, the ship leaves and the adults don't believe the children. Elsewhere, we see Vincent Schiavelli, whom I find to be a terrific actor (watch his scenes in `Ghost' for proof, as they are outstanding), who is playing a prospector, or as I called him, the Miner 1949er. He steps out of the cave he is in, and he and his dog are inked by the ship. Twenty-five years go by, and the girl has grown up to be Cybill Shepherd, who works with her father, Raymond Burr, in studying unusual weather phenomena. Or something like that. Shepherd spots some strange phenomena in satellite pictures over that little New Mexico town, and she travels there to research it. Once she gets there, the local ranchers harass her, and blame her for the recent slew of cattle mutilations that have been going on, and deputy Jan-Michael Vincent comes to her rescue. From this point on, the film really drags as the two quickly fall for each other, especially after Vincent wards off the locals and informs Shepherd that he was the little boy that saw the ship with her twenty-five years earlier. While this boring mess is happening, Vincent Schiavelli, with his killer dog at his side, is walking around killing the cattle and any people he runs into with an unusual item. You know those glowing plastic sticks stores sell for trick-or-treaters at Halloween, the kind that you shake to make them glow? Schiavelli uses what looks like one of those glow sticks to burn incisions in people. It's the second-worst effect in the movie. Every time Schiavelli is on screen with the glow stick, the scene's atmosphere suddenly turns dark, like the filmmakers thought the glow stick needed that enhancement. It ends up making the movie look even cheaper than it is.<br /><br />And what does all this lead up to? It's hard to tell when the final, confusing scene arrives. See, Burr and his team of scientists try to explain the satellite images that Shepherd found as some kind of `calling card,' but none of it makes sense. Why do Shepherd and Vincent age and Schiavelli does not? Schiavelli explains why he is killing cattle and people and why he wants Shepherd dead, but even that doesn't make much sense when you really think about it. I mean, why doesn't he kill Jan-Michael Vincent? After all, he had twenty-five years to do it. And the aliens won't need him if Shepherd is dead anyhow, so why try to kill her? Speaking of the aliens, it is never clear what they really wanted out of Shepherd and Vincent. What is their goal? Why do they wait so long to intervene? How could they be so sure Shepherd would come back? Not that the answer to any of these and other questions would have made `The Return' any more pleasant. You would still have bad lines, really bad acting, particularly by Shepherd, cheesy effects, and poor direction. Luckily, the stars escaped from this movie. Cybill Shepherd soon went on to star in `Moonlighting' with Bruce Willis. Jan-Michael Vincent went on to be featured in dozens of B-movies, often in over-the-top parts. Raymond Burr made a pile of Perry Mason television movies right up until his death. Vincent Schiavelli went on to be a great character actor in a huge number of films. Martin Landau, who played a kooky law enforcement officer, quickly made the terrific `Alone in the Dark' and the awful `The Being' before rolling into the films he has been famous for recently. You can bet none of these stars ever want their careers to return to `The Return.' Zantara's score: 2 out of 10.\",\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][42]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e4a8c6-e290-428a-85c5-5b942340c56f",
   "metadata": {},
   "source": [
    "# Huggingface Trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bd4abfc-6414-495e-b3d7-95aaaccd1562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimdb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m tokenized_datasets \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(tokenize_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 20\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     27\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     28\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     29\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     30\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     32\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m<string>:134\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/Repositories/udacity_generative_ai_nanodegree/project1_fine_tuning_foundational_model/.venv/lib/python3.12/site-packages/transformers/training_args.py:1772\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1770\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[1;32m   1771\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[0;32m-> 1772\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;66;03m# Disable average tokens when using single device\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage_tokens_across_devices:\n",
      "File \u001b[0;32m~/Documents/Repositories/udacity_generative_ai_nanodegree/project1_fine_tuning_foundational_model/.venv/lib/python3.12/site-packages/transformers/training_args.py:2294\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2290\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2291\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[1;32m   2292\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2293\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m-> 2294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Repositories/udacity_generative_ai_nanodegree/project1_fine_tuning_foundational_model/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:62\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     60\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[0;32m~/Documents/Repositories/udacity_generative_ai_nanodegree/project1_fine_tuning_foundational_model/.venv/lib/python3.12/site-packages/transformers/training_args.py:2167\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   2166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 2167\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2168\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2169\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2170\u001b[0m         )\n\u001b[1;32m   2171\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[1;32m   2172\u001b[0m accelerator_state_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "from transformers import (DistilBertForSequenceClassification,\n",
    "    DistilBertTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=64,\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d171146-db23-436b-bb39-df686048d03a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
